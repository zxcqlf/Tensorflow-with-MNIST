{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-cdd87b147cb0>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/zhaocq/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/zhaocq/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/zhaocq/桌面/tensorflow/mnist/raw/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/zhaocq/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/zhaocq/桌面/tensorflow/mnist/raw/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/zhaocq/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/zhaocq/桌面/tensorflow/mnist/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/zhaocq/桌面/tensorflow/mnist/raw/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/zhaocq/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/home/zhaocq/桌面/tensorflow/mnist/raw/\",one_hot=True)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "batchs = 64\n",
    "df_dim = 64#鉴别器通道\n",
    "gf_dim = 64#生成器通道\n",
    "z_dim = 100\n",
    "output_height = 28\n",
    "output_weight = 28\n",
    "image_dims = [output_height,output_weight,1]\n",
    "inputs = tf.placeholder(tf.float32,[batchs]+image_dims,name = \"real_images\")\n",
    "z = tf.placeholder(tf.float32,[None,z_dim],name = 'z')\n",
    "x = tf.placeholder(tf.float32,[batchs,784],name = 'x')\n",
    "x_image = tf.reshape(x ,[batchs,28,28,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(input_,output_dim,k_h=5,k_w=5,d_h=2,d_w=2,steddev=0.02,name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w',[k_h,k_w,input_.get_shape()[-1],output_dim],\\\n",
    "                            initializer= tf.truncated_normal_initializer(stddev=steddev))\n",
    "        conv = tf.nn.conv2d(input_,w,strides=[1,d_h,d_w,1],padding='SAME')\n",
    "        biases = tf.get_variable('biases',[output_dim],initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv,biases),conv.get_shape())\n",
    "        return conv\n",
    "\n",
    "def deconv2d(input_,output_shape,k_h=5,k_w=5,d_h=2,d_w=2,steddev=0.02,name=\"deconv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w',[k_h,k_w,output_shape[-1],input_.get_shape()[-1]],\\\n",
    "                           initializer=tf.random_normal_initializer(stddev=steddev))\n",
    "        deconv = tf.nn.conv2d_transpose(input_,w,output_shape=output_shape,strides=[1,d_h,d_w,1])\n",
    "        biases = tf.get_variable('biases',[output_shape[-1]],initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv,biases),deconv.get_shape())\n",
    "        return deconv\n",
    "\n",
    "def linear(input_,output_size,scope=None,steddev=0.02):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"matrix\",[shape[1],output_size],tf.float32,tf.random_normal_initializer(stddev=steddev))\n",
    "        bias = tf.get_variable(\"bias\",[output_size],initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input_,matrix) + bias\n",
    "\n",
    "def conv_out_size_same(size,stride):\n",
    "    return int(math.ceil(float(size) / float(stride)))\n",
    "\n",
    "def discriminator(image,reuse = False):\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        h0 = tf.nn.leaky_relu(conv2d(image,df_dim,name='d_h0_conv'))\n",
    "        h1 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(conv2d(h0,df_dim*2,name='d_h1_conv')))\n",
    "        h2 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(conv2d(h1,df_dim*4,name='d_h2_conv')))\n",
    "        h3 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(conv2d(h2,df_dim*8,name='d_h3_conv')))\n",
    "        h4 = linear(tf.reshape(h3,[batchs,-1]),1,'d_h4_lin')\n",
    "        return tf.nn.sigmoid(h4),h4\n",
    "def generator(z):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        s_h,s_w = output_height,output_weight\n",
    "        s_h2,s_w2 = conv_out_size_same(s_h,2),conv_out_size_same(s_w,2)\n",
    "        s_h4,s_w4 = conv_out_size_same(s_h2,2),conv_out_size_same(s_w2,2)\n",
    "        s_h8,s_w8 = conv_out_size_same(s_h4,2),conv_out_size_same(s_w4,2)\n",
    "        s_h16,s_w16 = conv_out_size_same(s_h8,2),conv_out_size_same(s_w8,2)\n",
    "        \n",
    "        z_ = linear(z,gf_dim*8*s_h16*s_w16,'g_h0_lin')\n",
    "        h0 = tf.reshape(z_,[-1,s_h16,s_w16,gf_dim*8])\n",
    "        h0 = tf.nn.relu(tf.contrib.layers.batch_norm(h0))\n",
    "        h1 = deconv2d(h0,[batchs,s_h8,s_w8,gf_dim*4],name='g_h1')\n",
    "        h1 = tf.nn.relu(tf.contrib.layers.batch_norm(h1))\n",
    "        h2 = deconv2d(h1,[batchs,s_h4,s_w4,gf_dim*2],name='g_h2')\n",
    "        h2 = tf.nn.relu(tf.contrib.layers.batch_norm(h2))\n",
    "        h3 = deconv2d(h2,[batchs,s_h2,s_w2,gf_dim*1],name='g_h3')\n",
    "        h3 = tf.nn.relu(tf.contrib.layers.batch_norm(h3))\n",
    "        h4 = deconv2d(h3,[batchs,s_h,s_w,1],name='g_h4')\n",
    "        return tf.nn.sigmoid(h4)\n",
    "\n",
    "    \n",
    "g_sample = generator(z)\n",
    "d_real,d_log_real = discriminator(x_image,reuse=False)\n",
    "d_fake,d_log_fake = discriminator(g_sample,reuse=True)\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_log_real,labels=tf.ones_like(d_real)) + \\\n",
    "                        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_log_fake,labels=tf.zeros_like(d_fake)))#交叉熵损失\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_log_fake,labels=tf.ones_like(d_fake)))\n",
    "#定义原始损失\n",
    "#d_loss = - tf.reduce_mean(tf.log(d_real) + tf.log(1.- d_fake))\n",
    "#g_loss =  - tf.reduce_mean(tf.log(d_fake))\n",
    "#g_loss =  tf.reduce_mean(tf.square(1.-d_fake)-tf.log(d_fake))\n",
    "d_slover = tf.train.AdamOptimizer(0.0002,beta1=0.5).minimize(d_loss,var_list = d_vars)\n",
    "g_slover = tf.train.AdamOptimizer().minimize(g_loss,var_list = g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 d loss : 0.1131 g loss : 4.98\n",
      "iter:1 d loss : 0.02829 g loss : 7.473\n",
      "iter:2 d loss : 0.439 g loss : 5.176\n",
      "iter:3 d loss : 0.2809 g loss : 4.791\n",
      "iter:4 d loss : 0.255 g loss : 2.601\n",
      "iter:5 d loss : 0.8293 g loss : 0.2511\n",
      "iter:6 d loss : 0.2299 g loss : 5.164\n",
      "iter:7 d loss : 0.09369 g loss : 3.981\n",
      "iter:8 d loss : 0.09259 g loss : 8.178\n",
      "iter:9 d loss : 0.3603 g loss : 2.516\n",
      "iter:10 d loss : 0.09593 g loss : 3.587\n",
      "iter:11 d loss : 0.02535 g loss : 5.384\n"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "i=0\n",
    "for epoch in range (12):\n",
    "    for it in range(800):\n",
    "    #输出image\n",
    "        sample_z = np.random.uniform(-1, 1, [batchs, z_dim]).astype(np.float32)\n",
    "        x_md,_ =mnist.train.next_batch(batchs)\n",
    "        _,g_loss_curr = sess.run([g_slover,g_loss],feed_dict={z: sample_z})\n",
    "        #d_loss_curr = sess.run(d_loss,feed_dict={x: x_md, z: sample_z})\n",
    "        _,d_loss_curr = sess.run([d_slover,d_loss],feed_dict={x: x_md, z: sample_z})\n",
    "        _,g_loss_curr = sess.run([g_slover,g_loss],feed_dict={z: sample_z})\n",
    "        _,g_loss_curr = sess.run([g_slover,g_loss],feed_dict={z: sample_z})\n",
    "        _,g_loss_curr = sess.run([g_slover,g_loss],feed_dict={z: sample_z})\n",
    "    if epoch % 1 == 0:\n",
    "        #print('iter:{}'.format(epoch),'g loss : {:.4}'.format(g_loss_curr))\n",
    "        sampl = sess.run(g_sample,feed_dict={z: sample_z})\n",
    "        samples = sampl[:16]\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(5)),bbox_inches = 'tight')\n",
    "        i+=1\n",
    "        plt.close(fig)\n",
    "        print('iter:{}'.format(epoch),'d loss : {:.4}'.format(d_loss_curr),'g loss : {:.4}'.format(g_loss_curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
